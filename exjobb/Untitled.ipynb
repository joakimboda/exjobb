{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import Bio\n",
    "from Bio.PDB import *\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#This is for removing Bio.python warnings\n",
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "warnings.simplefilter('ignore', BiopythonWarning)\n",
    "\n",
    "#import pandas as pd #pandas Ã¤r en python modul for att hantera s.k DataFrames.\n",
    " \n",
    "        \n",
    "def find_structure_params(structure):\n",
    "    \n",
    "    #structure_data.append([Chain_id,Seg_id, residue_name, atom_name, atom_coord_vector])\n",
    "    structure_data={}\n",
    "    res_key = ''\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            \n",
    "            #for atom in structure.get_atoms():  If I want all atoms in a structure, depends if I want the residue\n",
    "            for residue in chain:\n",
    "                if residue.get_full_id()[3][0]==' ':  #If I want to remove HOH etc (hetero-atoms) use this!\n",
    "                    for atom in residue:\n",
    "                        key = chain.get_id() + str(atom.serial_number)\n",
    "                        structure_data[key]=([chain.get_id(),residue.get_resname(),atom.get_name(),atom.get_vector()])\n",
    "                        \n",
    "                        #Giving the C-terminal O its own name for when making the atom layers\n",
    "                        if atom.get_name()=='OXT':\n",
    "                            structure_data[res_key][2]=structure_data[res_key][2]+'Cterm'\n",
    "                        res_key = key\n",
    "   # for i in structure_data:\n",
    "       # if i[0]=='L':\n",
    "     #       print i\n",
    "\n",
    "\n",
    "    return (structure_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_atom_layers(structure_data):\n",
    "    \n",
    "    layer1_check = ['CYS:SG','MET:SD','MSE:SE']\n",
    "    layer2_check = ['ASN:ND2','GLN:NE2'] #Backbone N\n",
    "    layer3_check = ['HIS:ND1','HIS:NE1','TRP:NE1']\n",
    "    layer4_check = ['ARG:NE','ARG:NH1','ARG:NH2','ARG:NH3']\n",
    "    layer5_check = ['LYS:NZ']\n",
    "    layer6_check = ['ASN:OD1','GLN:OE1'] #Backbone O?\n",
    "    layer7_check = ['SER:OG','THR:OG1','TYR:OH']\n",
    "    layer8_check = ['ASP:OD1','ASP:OD2','ASP:OD3','GLU:OE1','GLU:OE2','GLU:OE3']\n",
    "    layer9_check = ['ARG:CZ','ASN:CG','ASP:CG','GLN:CD','GLU:CD'] #Backbone C?\n",
    "    layer10_check = ['HIS:CG','HIS:CD2','HIS:CE1','PHE:CG','PHE:CD1','PHE:CD2','PHE:CD3','PHE:CE1','PHE:CE2','PHE:CE3','PHE:CZ','TRP:CG','TRP:CD1','TRP:CD2','TRP:CD3','TRP:CD3','TRP:CE1','TRP:CE2','TRP:CE3','TRP:CZ1','TRP:CZ2','TRP:CZ3','TRP:CH2','TYR:CG','TYR:CD1','TYR:CD2','TYR:CD3','TYR:CE1','TYR:CE2','TYR:CE3','TYR:CZ']\n",
    "    layer11_check = ['ALA:CB','ARG:CB','ARG:CG','ARG:CD','ASN:CB','ASP:CB','CYS:CB','GLN:CB','GLN:CG','GLU:CB','GLU:CG','HIS:CB','ILE:CB','ILE:CG1','ILE:CG2','ILE:CG3','ILE:CD1','LEU:CB','LEU:CG','LEU:CD1','LEU:CD2','LEU:CD3','LYS:CB','LYS:CG','LYS:CD','LYS:CE','MET:CB','MET:CG','MET:CE','MSE:CB','MSE:CG','MSE:CE','PHE:CB','PRO:CB','PRO:CG','PRO:CD','SER:CB','THR:CB','THR:CG2','TRP:CB','TYR:CB','VAL:CB','VAL:CG1','VAL:CG2','VAL:CG3'] #Backbone CA?\n",
    "    \n",
    "    layers={}\n",
    "    \n",
    "    for key, atom in structure_data.items():\n",
    "        if atom[2] == 'N':\n",
    "            layers.setdefault('2', []).append(atom)\n",
    "        elif atom[2] == 'O':\n",
    "            layers.setdefault('6', []).append(atom)\n",
    "        elif atom[2] == 'C':\n",
    "            layers.setdefault('9', []).append(atom)\n",
    "        elif atom[2] == 'CA':\n",
    "            layers.setdefault('11', []).append(atom)\n",
    "        elif atom[2] == 'OXT':\n",
    "            layers.setdefault('8', []).append(atom)\n",
    "        elif atom[2] == 'OCterm': #C-terminal O\n",
    "            layers.setdefault('8', []).append(atom)\n",
    "        else:\n",
    "            atom_type = atom[1] + ':' + atom[2]\n",
    "            if atom_type in layer1_check:\n",
    "                layers.setdefault('1', []).append(atom)\n",
    "            elif atom_type in layer2_check:\n",
    "                layers.setdefault('2', []).append(atom)\n",
    "            elif atom_type in layer3_check:\n",
    "                layers.setdefault('3', []).append(atom)\n",
    "            elif atom_type in layer4_check:\n",
    "                layers.setdefault('4', []).append(atom)\n",
    "            elif atom_type in layer5_check:\n",
    "                layers.setdefault('5', []).append(atom)\n",
    "            elif atom_type in layer6_check:\n",
    "                layers.setdefault('6', []).append(atom)\n",
    "            elif atom_type in layer7_check:\n",
    "                layers.setdefault('7', []).append(atom)\n",
    "            elif atom_type in layer8_check:\n",
    "                layers.setdefault('8', []).append(atom)\n",
    "            elif atom_type in layer9_check:\n",
    "                layers.setdefault('9', []).append(atom)\n",
    "            elif atom_type in layer10_check:\n",
    "                layers.setdefault('10', []).append(atom)\n",
    "            elif atom_type in layer11_check:\n",
    "                layers.setdefault('11', []).append(atom)\n",
    "            else:\n",
    "                d=1# print str(key) + str(atom[1:3]) + ' Has not been assigned to any layer' #layers.setdefault('12', []).append(atom)\n",
    "    \n",
    "    #This is to check if anything is not sorted into a layer            \n",
    "    #for key,atom in layers.items():\n",
    "        #if key=='12':\n",
    "           # for x in atom:\n",
    "             #   print x\n",
    "\n",
    "    return(layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_midpoint(structure_data):\n",
    "    \n",
    "    allvectors = Vector([0,0,0])\n",
    "\t\n",
    "    i=0     \n",
    "    for key, atom in structure_data.items():\n",
    "        allvectors=allvectors+atom[3]\n",
    "        i=i+1\n",
    "    midpoint=Vector([allvectors[0]/i,allvectors[1]/i,allvectors[2]/i])\n",
    "    return(midpoint)    \n",
    "\n",
    "\n",
    "\n",
    "def normalize_in_origo(midpoint,structure_data):\n",
    "\n",
    "    for key,atom in structure_data.items():\n",
    "        newvector = atom[3] - midpoint\n",
    "        atom_new=[atom[0],atom[1],atom[2],newvector]\n",
    "        structure_data[key]=atom_new\n",
    "\n",
    "    return (structure_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_density_maps(layers,density_maps,counter):\n",
    "    \n",
    "        for g in range(1, 12):\n",
    "            density_maps[counter].append(np.zeros((120,120,120)))\n",
    "            #density_maps[counter]=np.append(np.zeros((120,120,120)))\n",
    "        density_maps[counter]=np.array(density_maps[counter])    \n",
    "        for key,atom in layers.items():\n",
    "            for pos in atom:\n",
    "                x=int(np.around(pos[3][0], decimals=0))+60\n",
    "                y=int(np.around(pos[3][1], decimals=0))+60\n",
    "                z=int(np.around(pos[3][2], decimals=0))+60\n",
    "                \n",
    "                for xx in range(-2,3):\n",
    "                    for yy in range(-2,3):\n",
    "                        for zz in range(-2,3):\n",
    "                            r=float(max(abs(xx),abs(yy),abs(zz)))\n",
    "                            density=math.exp(-((r**2)/2))\n",
    "                            density_maps[counter][(int(key)-1)][x+xx][y+yy][z+zz]=(density_maps[counter][(int(key)-1)][x+xx][y+yy][z+zz]+density)\n",
    "\n",
    "        #This is only for plotting the data in python, otherwise a numpy array is made over all layers\n",
    "        x_values=[]\n",
    "        y_values=[]\n",
    "        z_values=[]\n",
    "        density_values = []\n",
    "        \n",
    "        for x in range(0,120):\n",
    "            for y in range(0,120):\n",
    "                for z in range(0,120):\n",
    "                    if density_maps[counter][1][x][y][z]>0.0:\n",
    "                        x_values.append(x)\n",
    "                        y_values.append(y)\n",
    "                        z_values.append(z)\n",
    "                        density_values.append(density_maps[counter][1][x][y][z])\n",
    "        \n",
    "        return (density_maps,x_values,y_values,z_values,density_values)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_learning(density_maps):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.convolutional import Conv3D\n",
    "    from keras.layers import Conv3D, MaxPooling3D,Activation,Reshape\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.optimizers import Adam\n",
    "    \n",
    "    import keras.backend as K #For compile\n",
    "    \n",
    "    print('Start training')\n",
    "    seq = Sequential()\n",
    "\n",
    "    #seq.add(Conv3D(11, 3, 3, 3, activation='relu', \n",
    "                            #border_mode='valid', name='conv1',\n",
    "                            #subsample=(1, 1, 1),\n",
    "\t\t\t    #dim_ordering='th', \n",
    "                            #input_shape=(11,120, 120, 120)))\n",
    "\n",
    "\n",
    "    #seq.add(Conv3D(filters=11, kernel_size=(3,3,3), strides=(1,1,1), activation='relu',padding='valid', data_format='channels_first', input_shape=(11,120, 120, 120)))\n",
    "\n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2),data_format='channels_first'))\n",
    "\n",
    "    \n",
    "    #seq.add(Conv3D(filters=16, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "    \n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2)))\n",
    "\n",
    "    \n",
    "    #seq.add(Conv3D(filters=32, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=32, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "   \n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2)))\n",
    "\n",
    "\n",
    "    #seq.add(Conv3D(filters=64, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=128, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=128, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=256, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2)))\n",
    "\n",
    "    \n",
    "    #seq.add(Reshape((-1,)))\n",
    "\n",
    "    #seq.add(Activation('linear'))\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Activation('linear'))\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Activation('linear'))\n",
    "\n",
    "\n",
    "    \n",
    "    def mean_pred(y_true, y_pred):\n",
    "        return K.mean(y_pred)\n",
    "    \n",
    "\n",
    "\n",
    "    adam = Adam(lr=0.0003, decay=0.01)\n",
    "    seq.compile(loss='binary_crossentropy',\n",
    "\t      optimizer=adam,\n",
    "              metrics=['accuracy', mean_pred])\n",
    "    class_y = np.random.random((10, 11, 118,118,118))\n",
    "\t\n",
    "    seq.fit(density_maps,class_y,\n",
    "          epochs=20,\n",
    "          batch_size=9)\n",
    "    \n",
    "    print('Training done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "D1CGI\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1DQJ/D1DQJ-a0a-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1DQJ/D1DQJ-a0b-merged.pdb\n",
      "2/2, 100.0%\n",
      "(2, 11, 120, 120, 120)\n",
      "(1, 11, 120, 120, 120)\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1AHW/D1AHW-a0b-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1AHW/D1AHW-a0a-merged.pdb\n",
      "2/2, 100.0%\n",
      "(2, 11, 120, 120, 120)\n",
      "(3, 11, 120, 120, 120)\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1CLV/D1CLV-a0b-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1CLV/D1CLV-a0a-merged.pdb\n",
      "2/2, 100.0%\n",
      "(2, 11, 120, 120, 120)\n",
      "(5, 11, 120, 120, 120)\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1BVN/D1BVN-a0a-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1BVN/D1BVN-a0b-merged.pdb\n",
      "2/2, 100.0%\n",
      "(2, 11, 120, 120, 120)\n",
      "(7, 11, 120, 120, 120)\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1E6J/D1E6J-a0a-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1E6J/D1E6J-a0b-merged.pdb\n",
      "2/2, 100.0%\n",
      "(2, 11, 120, 120, 120)\n",
      "(9, 11, 120, 120, 120)\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1EAW/D1EAW-a0a-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1EAW/D1EAW-a0b-merged.pdb\n",
      "2/2, 100.0%\n",
      "(2, 11, 120, 120, 120)\n",
      "(11, 11, 120, 120, 120)\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1D6R/D1D6R-a0b-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1D6R/D1D6R-a0a-merged.pdb\n",
      "2/2, 100.0%\n",
      "(2, 11, 120, 120, 120)\n",
      "(13, 11, 120, 120, 120)\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1AVX/D1AVX-a0a-merged.pdb\n",
      "1/2, 50.0%\n",
      "/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/D1AVX/D1AVX-a0b-merged.pdb\n",
      "2/2, 100.0%\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    #Dir path\n",
    "    args = sys.argv[1:]\n",
    "\n",
    "    \n",
    "    files='/home/joakim/Downloads/models/' #str(args[0])\n",
    "\n",
    "\n",
    "\n",
    "    df=pd.read_excel('/home/joakim/Downloads/cross_val_sets.xls')\n",
    "    targets={}\n",
    "    for i,row in df.iterrows():\n",
    "         targets.setdefault(i, []).append(row['Targets'].split())\n",
    "\n",
    "    datasets={}\n",
    "    for dir in os.listdir('/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark/'):\n",
    "        for key,target in targets.items():\n",
    "            if dir in str(target):\n",
    "                datasets.setdefault(key, []).append(dir)\n",
    "\n",
    "    print type(targets[1][0][1])\n",
    "    print targets[1][0][1]\n",
    "\n",
    "    \n",
    "\n",
    "            \n",
    "    df=pd.read_excel('/home/joakim/Downloads/cross_val_sets.xls')\n",
    "    targets={}\n",
    "    for i,row in df.iterrows():\n",
    "         targets.setdefault(i, []).append(row['Targets'].split())\n",
    "\n",
    "    dir_home='/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/'\n",
    "    datasets={}\n",
    "\n",
    "    for dir in os.listdir(dir_home):\n",
    "        for key,target in targets.items():\n",
    "            if dir in str(target):\n",
    "                datasets.setdefault(key, []).append(dir)\n",
    "\n",
    "    protein_maps=np.empty([1, 11, 120, 120, 120])\n",
    "    for key,protein_list in datasets.items():\n",
    "        \n",
    "        for prot_counter,protein in enumerate(protein_list):\n",
    "            \n",
    "\n",
    "            pdb_list=[]\n",
    "            for file in os.listdir(dir_home + protein):\n",
    "                if file.endswith(\".pdb\"):\n",
    "                    pdb_list.append(dir_home + protein+'/'+file)\n",
    "               \n",
    "            density_maps=[] \n",
    "            if not '-r' in args:\n",
    "                for counter, filename_pdb in enumerate(pdb_list):\n",
    "                    print(filename_pdb)\n",
    "                    percent= np.around((np.float(counter+1) / len(pdb_list)*100), decimals=3)\n",
    "                    print(str((counter+1)) +'/'+ str(len(pdb_list)) + ', ' + str(percent)+'%')\n",
    "                    density_maps.append([])\n",
    "\n",
    "                    #filename_pdb = '/home/joakim/Downloads/5eh6.pdb'#'/home/joakim/Downloads/2HIY_A.pdb' #'/home/joakim/Downloads/D1A2K-a0a-merged.pdb'\n",
    "                    try: \n",
    "                        PDBobj = PDBParser()\n",
    "                        structure = PDBobj.get_structure(filename_pdb, filename_pdb)\n",
    "\n",
    "                    except IOError: \n",
    "                        print('IO Error', filename_pdb)       \n",
    "                        while 'true':\n",
    "                            input1=raw_input(\"Error parsing PDB file! Continue(y/n)...\")\n",
    "                            if input1.lower()=='y':\n",
    "                                break\n",
    "                            elif input1.lower()=='n':\n",
    "                                sys.exit()\n",
    "\n",
    "\n",
    "                    structure_data = find_structure_params(structure)\n",
    "\n",
    "                    midpoint = find_midpoint(structure_data) #Avrundning gÃ¶r att det blir lite konstigt,!! Kolla upp\n",
    "\n",
    "                    structure_data = normalize_in_origo(midpoint,structure_data)\n",
    "\n",
    "                    layers = make_atom_layers(structure_data)\n",
    "\n",
    "\n",
    "\n",
    "                    #Create 11 density maps (zeros)\n",
    "                    (density_maps,x_values,y_values,z_values,density_values) = make_density_maps(layers,density_maps,counter)\n",
    "\n",
    "            \n",
    "            density_maps=np.array(density_maps)\n",
    "            print(density_maps.shape)\n",
    "            print(protein_maps.shape)\n",
    "            protein_maps=np.concatenate((protein_maps, density_maps), axis=0)\n",
    "           # protein_maps.append(density_maps)\n",
    "\n",
    "        #protein_maps=np.array(density_maps)\n",
    "#For saving and loading the array as an compressed npz file\n",
    "    if '-s' in args and '-r' in args:\n",
    "        print('Do not use save(-s) and read(-r) at the same time')\n",
    "        sys.exit()\n",
    "    elif '-s' in args:\n",
    "        print('-Saving file...')\n",
    "        np.savez_compressed('density_maps', protein_maps)\n",
    "        print('-File saved as \"density_maps.npz\"')\n",
    "\n",
    "    elif '-r' in args:\n",
    "        print('-Loading file...')\n",
    "        protein_maps = np.load('density_maps.npz')\n",
    "        for key,array in protein_maps.items():\n",
    "            protein_maps=protein_maps[key]\n",
    "        print('-File loaded')\n",
    "\n",
    "    print(type(protein_maps))\n",
    "    print(type(protein_maps[0]))\n",
    "    print(type(protein_maps[0][0]))\n",
    "    print(type(protein_maps[0][0][0]))\n",
    "    print(type(protein_maps[0][0][0][0]))\n",
    "    print(type(protein_maps[0][0][0][0][0]))\n",
    "\t\t\n",
    "    deep_learning(protein_maps)\n",
    "        \n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
