{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import Bio\n",
    "from Bio.PDB import *\n",
    "import sys\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#This is for removing Bio.python warnings\n",
    "import warnings\n",
    "from Bio import BiopythonWarning\n",
    "warnings.simplefilter('ignore', BiopythonWarning)\n",
    "\n",
    "#import pandas as pd #pandas Ã¤r en python modul for att hantera s.k DataFrames.\n",
    " \n",
    "        \n",
    "def find_structure_params(structure):\n",
    "    \n",
    "    #structure_data.append([Chain_id,Seg_id, residue_name, atom_name, atom_coord_vector])\n",
    "    structure_data={}\n",
    "    res_key = ''\n",
    "    \n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            \n",
    "            #for atom in structure.get_atoms():  If I want all atoms in a structure, depends if I want the residue\n",
    "            for residue in chain:\n",
    "                if residue.get_full_id()[3][0]==' ':  #If I want to remove HOH etc (hetero-atoms) use this!\n",
    "                    for atom in residue:\n",
    "                        key = chain.get_id() + str(atom.serial_number)\n",
    "                        structure_data[key]=([chain.get_id(),residue.get_resname(),atom.get_name(),atom.get_vector()])\n",
    "                        \n",
    "                        #Giving the C-terminal O its own name for when making the atom layers\n",
    "                        if atom.get_name()=='OXT':\n",
    "                            structure_data[res_key][2]=structure_data[res_key][2]+'Cterm'\n",
    "                        res_key = key\n",
    "   # for i in structure_data:\n",
    "       # if i[0]=='L':\n",
    "     #       print i\n",
    "\n",
    "\n",
    "    return (structure_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_atom_layers(structure_data):\n",
    "    \n",
    "    layer1_check = ['CYS:SG','MET:SD','MSE:SE']\n",
    "    layer2_check = ['ASN:ND2','GLN:NE2'] #Backbone N\n",
    "    layer3_check = ['HIS:ND1','HIS:NE1','TRP:NE1']\n",
    "    layer4_check = ['ARG:NE','ARG:NH1','ARG:NH2','ARG:NH3']\n",
    "    layer5_check = ['LYS:NZ']\n",
    "    layer6_check = ['ASN:OD1','GLN:OE1'] #Backbone O?\n",
    "    layer7_check = ['SER:OG','THR:OG1','TYR:OH']\n",
    "    layer8_check = ['ASP:OD1','ASP:OD2','ASP:OD3','GLU:OE1','GLU:OE2','GLU:OE3']\n",
    "    layer9_check = ['ARG:CZ','ASN:CG','ASP:CG','GLN:CD','GLU:CD'] #Backbone C?\n",
    "    layer10_check = ['HIS:CG','HIS:CD2','HIS:CE1','PHE:CG','PHE:CD1','PHE:CD2','PHE:CD3','PHE:CE1','PHE:CE2','PHE:CE3','PHE:CZ','TRP:CG','TRP:CD1','TRP:CD2','TRP:CD3','TRP:CD3','TRP:CE1','TRP:CE2','TRP:CE3','TRP:CZ1','TRP:CZ2','TRP:CZ3','TRP:CH2','TYR:CG','TYR:CD1','TYR:CD2','TYR:CD3','TYR:CE1','TYR:CE2','TYR:CE3','TYR:CZ']\n",
    "    layer11_check = ['ALA:CB','ARG:CB','ARG:CG','ARG:CD','ASN:CB','ASP:CB','CYS:CB','GLN:CB','GLN:CG','GLU:CB','GLU:CG','HIS:CB','ILE:CB','ILE:CG1','ILE:CG2','ILE:CG3','ILE:CD1','LEU:CB','LEU:CG','LEU:CD1','LEU:CD2','LEU:CD3','LYS:CB','LYS:CG','LYS:CD','LYS:CE','MET:CB','MET:CG','MET:CE','MSE:CB','MSE:CG','MSE:CE','PHE:CB','PRO:CB','PRO:CG','PRO:CD','SER:CB','THR:CB','THR:CG2','TRP:CB','TYR:CB','VAL:CB','VAL:CG1','VAL:CG2','VAL:CG3'] #Backbone CA?\n",
    "    \n",
    "    layers={}\n",
    "    \n",
    "    for key, atom in structure_data.items():\n",
    "        if atom[2] == 'N':\n",
    "            layers.setdefault('2', []).append(atom)\n",
    "        elif atom[2] == 'O':\n",
    "            layers.setdefault('6', []).append(atom)\n",
    "        elif atom[2] == 'C':\n",
    "            layers.setdefault('9', []).append(atom)\n",
    "        elif atom[2] == 'CA':\n",
    "            layers.setdefault('11', []).append(atom)\n",
    "        elif atom[2] == 'OXT':\n",
    "            layers.setdefault('8', []).append(atom)\n",
    "        elif atom[2] == 'OCterm': #C-terminal O\n",
    "            layers.setdefault('8', []).append(atom)\n",
    "        else:\n",
    "            atom_type = atom[1] + ':' + atom[2]\n",
    "            if atom_type in layer1_check:\n",
    "                layers.setdefault('1', []).append(atom)\n",
    "            elif atom_type in layer2_check:\n",
    "                layers.setdefault('2', []).append(atom)\n",
    "            elif atom_type in layer3_check:\n",
    "                layers.setdefault('3', []).append(atom)\n",
    "            elif atom_type in layer4_check:\n",
    "                layers.setdefault('4', []).append(atom)\n",
    "            elif atom_type in layer5_check:\n",
    "                layers.setdefault('5', []).append(atom)\n",
    "            elif atom_type in layer6_check:\n",
    "                layers.setdefault('6', []).append(atom)\n",
    "            elif atom_type in layer7_check:\n",
    "                layers.setdefault('7', []).append(atom)\n",
    "            elif atom_type in layer8_check:\n",
    "                layers.setdefault('8', []).append(atom)\n",
    "            elif atom_type in layer9_check:\n",
    "                layers.setdefault('9', []).append(atom)\n",
    "            elif atom_type in layer10_check:\n",
    "                layers.setdefault('10', []).append(atom)\n",
    "            elif atom_type in layer11_check:\n",
    "                layers.setdefault('11', []).append(atom)\n",
    "            else:\n",
    "                d=1# print str(key) + str(atom[1:3]) + ' Has not been assigned to any layer' #layers.setdefault('12', []).append(atom)\n",
    "    \n",
    "    #This is to check if anything is not sorted into a layer            \n",
    "    #for key,atom in layers.items():\n",
    "        #if key=='12':\n",
    "           # for x in atom:\n",
    "             #   print x\n",
    "\n",
    "    return(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_midpoint(structure_data):\n",
    "    \n",
    "    allvectors = Vector([0,0,0])\n",
    "\n",
    "    i=0     \n",
    "    for key, atom in structure_data.items():\n",
    "        allvectors=allvectors+atom[3]\n",
    "        i=i+1\n",
    "    midpoint=Vector([allvectors[0]/i,allvectors[1]/i,allvectors[2]/i])\n",
    "    return(midpoint)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_in_origo(midpoint,structure_data):\n",
    "\n",
    "    for key,atom in structure_data.items():\n",
    "        newvector = atom[3] - midpoint\n",
    "        atom_new=[atom[0],atom[1],atom[2],newvector]\n",
    "        structure_data[key]=atom_new\n",
    "\n",
    "    return (structure_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_density_maps(layers,density_maps,counter):\n",
    "    \n",
    "        for g in range(1, 12):\n",
    "            density_maps[counter].append(np.zeros((120,120,120)))\n",
    "            #density_maps[counter]=np.append(np.zeros((120,120,120)))\n",
    "        density_maps[counter]=np.array(density_maps[counter])    \n",
    "        for key,atom in layers.items():\n",
    "            for pos in atom:\n",
    "                x=int(np.around(pos[3][0], decimals=0))+60\n",
    "                y=int(np.around(pos[3][1], decimals=0))+60\n",
    "                z=int(np.around(pos[3][2], decimals=0))+60\n",
    "                \n",
    "                for xx in range(-2,3):\n",
    "                    for yy in range(-2,3):\n",
    "                        for zz in range(-2,3):\n",
    "                            r=float(max(abs(xx),abs(yy),abs(zz)))\n",
    "                            density=math.exp(-((r**2)/2))\n",
    "                            density_maps[counter][(int(key)-1)][x+xx][y+yy][z+zz]=(density_maps[counter][(int(key)-1)][x+xx][y+yy][z+zz]+density)\n",
    "\n",
    "        #This is only for plotting the data in python, otherwise a numpy array is made over all layers\n",
    "        x_values=[]\n",
    "        y_values=[]\n",
    "        z_values=[]\n",
    "        density_values = []\n",
    "        \n",
    "        for x in range(0,120):\n",
    "            for y in range(0,120):\n",
    "                for z in range(0,120):\n",
    "                    if density_maps[counter][1][x][y][z]>0.0:\n",
    "                        x_values.append(x)\n",
    "                        y_values.append(y)\n",
    "                        z_values.append(z)\n",
    "                        density_values.append(density_maps[counter][1][x][y][z])\n",
    "        \n",
    "        return (density_maps,x_values,y_values,z_values,density_values)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_learning(protein_train,protein_test,protein_target):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.convolutional import Conv3D\n",
    "    from keras.layers import Conv3D, MaxPooling3D,Activation,Reshape\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.optimizers import Adam\n",
    "    \n",
    "    import keras.backend as K #For compile\n",
    "    \n",
    "    print('Start training')\n",
    "    seq = Sequential()\n",
    "\n",
    "    #seq.add(Conv3D(11, 3, 3, 3, activation='relu', \n",
    "                            #border_mode='valid', name='conv1',\n",
    "                            #subsample=(1, 1, 1),\n",
    "                            #dim_ordering='th', \n",
    "                            #input_shape=(11,120, 120, 120)))\n",
    "\n",
    "\n",
    "    seq.add(Conv3D(filters=11, kernel_size=(3,3,3), strides=(1,1,1), activation='relu',padding='valid', data_format='channels_first', input_shape=(15,1,11,120, 120, 120)))\n",
    "\n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2),data_format='channels_first'))\n",
    "\n",
    "    \n",
    "    #seq.add(Conv3D(filters=16, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "    \n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2)))\n",
    "\n",
    "    \n",
    "    #seq.add(Conv3D(filters=32, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=32, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "   \n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2)))\n",
    "\n",
    "\n",
    "    #seq.add(Conv3D(filters=64, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=128, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=128, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Conv3D(filters=256, kernel_size=(3,3,3), strides=(1,1,1),padding='same', data_format='channels_first'))\n",
    "\n",
    "    #seq.add(BatchNormalization())\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2)))\n",
    "\n",
    "    \n",
    "    #seq.add(Reshape((-1,)))\n",
    "\n",
    "    #seq.add(Activation('linear'))\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Activation('linear'))\n",
    "\n",
    "    #seq.add(Activation('relu'))\n",
    "\n",
    "    #seq.add(Activation('linear'))\n",
    "\n",
    "\n",
    "    \n",
    "    def mean_pred(y_true, y_pred):\n",
    "        return K.mean(y_pred)\n",
    "    \n",
    "\n",
    "\n",
    "    adam = Adam(lr=0.0003, decay=0.01)\n",
    "    seq.compile(loss='binary_crossentropy',\n",
    "            optimizer=adam,\n",
    "              metrics=['accuracy', mean_pred])\n",
    "\n",
    "\n",
    "    seq.fit(protein_train,protein_target,\n",
    "          epochs=20,\n",
    "          batch_size=9)\n",
    "    \n",
    "    print('Training done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save or load-r\n",
      "-Loading files...\n",
      "-Files loaded\n",
      "15\n",
      "15\n",
      "5\n",
      "Start training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer conv3d_10: expected ndim=5, found ndim=7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-582-f9bd0afe88b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-582-f9bd0afe88b3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mdeep_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotein_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotein_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprotein_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-581-4ecde49aaa17>\u001b[0m in \u001b[0;36mdeep_learning\u001b[0;34m(protein_train, protein_test, protein_target)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'channels_first'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#seq.add(MaxPooling3D(pool_size=(3,3,3),strides=(2,2,2),data_format='channels_first'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/joakim/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0;31m# and create the node connecting the current layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0;31m# to the input layer we just created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/joakim/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/joakim/anaconda2/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer conv3d_10: expected ndim=5, found ndim=7"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    \n",
    "    #Dir path\n",
    "    args = sys.argv[1:]\n",
    "\n",
    "    \n",
    "    files='/home/joakim/Downloads/models/' #str(args[0])\n",
    "    while 'true':\n",
    "        input1=raw_input(\"Save or load\")\n",
    "        if input1.lower()=='-r':\n",
    "            args='-r'\n",
    "            break\n",
    "        elif input1.lower()=='-s':\n",
    "            args='-s'\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    df=pd.read_excel('/home/joakim/Downloads/cross_val_sets.xls')\n",
    "    targets={}\n",
    "    for i,row in df.iterrows():\n",
    "         targets.setdefault(i, []).append(row['Targets'].split())\n",
    "\n",
    "    dir_home='/home/joakim/Downloads/CnM-dataset/MOAL_Benchmark_test/'\n",
    "    datasets={}\n",
    "\n",
    "    for dir in os.listdir(dir_home):\n",
    "        for key,target in targets.items():\n",
    "            if dir in str(target):\n",
    "                datasets.setdefault(key, []).append(dir)\n",
    "\n",
    "    protein_train=[]\n",
    "    protein_test=[]\n",
    "    protein_target=[]\n",
    "    prot_counter_train=1\n",
    "    target_counter=0\n",
    "    for key,protein_list in datasets.items():\n",
    "        \n",
    "        for prot_counter,protein in enumerate(protein_list):\n",
    "            \n",
    "\n",
    "            pdb_list=[]\n",
    "            for file in os.listdir(dir_home + protein):\n",
    "                if file.endswith(\".pdb\"):\n",
    "                    pdb_list.append(dir_home + protein+'/'+file)\n",
    "\n",
    "\n",
    "            density_maps=[]\n",
    "            counter=0\n",
    "\n",
    "            if not '-r' in args:\n",
    "                for filename_pdb in pdb_list:\n",
    "                    print(filename_pdb)\n",
    "                    percent= np.around((np.float(counter+1) / len(pdb_list)*100), decimals=3)\n",
    "                    #print(str((counter+1)) +'/'+ str(len(pdb_list)) + ', ' + str(percent)+'%')\n",
    "                    \n",
    "\n",
    "                    #filename_pdb = '/home/joakim/Downloads/5eh6.pdb'#'/home/joakim/Downloads/2HIY_A.pdb' #'/home/joakim/Downloads/D1A2K-a0a-merged.pdb'\n",
    "                    try: \n",
    "                        PDBobj = PDBParser()\n",
    "                        structure = PDBobj.get_structure(filename_pdb, filename_pdb)\n",
    "\n",
    "                    except IOError: \n",
    "                        print('IO Error', filename_pdb)       \n",
    "                        while 'true':\n",
    "                            input1=raw_input(\"Error parsing PDB file! Continue(y/n)...\")\n",
    "                            if input1.lower()=='y':\n",
    "                                break\n",
    "                            elif input1.lower()=='n':\n",
    "                                sys.exit()\n",
    "\n",
    "                    \n",
    "\n",
    "                    structure_data = find_structure_params(structure)\n",
    "\n",
    "                    midpoint = find_midpoint(structure_data) #Avrundning gÃ¶r att det blir lite konstigt,!! Kolla upp\n",
    "\n",
    "                    structure_data = normalize_in_origo(midpoint,structure_data)\n",
    "\n",
    "                    layers = make_atom_layers(structure_data)\n",
    "\n",
    "                    #Create 11 density maps (zeros)\n",
    "                    if 'a0a' in filename_pdb and prot_counter!=prot_counter_train:\n",
    "                        protein_target.append([])\n",
    "                        (protein_target,x_values,y_values,z_values,density_values) = make_density_maps(layers,protein_target,target_counter)\n",
    "                        target_counter=target_counter+1\n",
    "                    else:\n",
    "                        density_maps.append([])\n",
    "                        (density_maps,x_values,y_values,z_values,density_values) = make_density_maps(layers,density_maps,counter)\n",
    "                        counter=counter+1\n",
    "                        \n",
    "            if prot_counter==prot_counter_train:\n",
    "                protein_test.append(np.array(density_maps))\n",
    "            else:\n",
    "                protein_train.append(np.array(density_maps))\n",
    "    \n",
    "    protein_target=np.array(protein_target)\n",
    "    protein_train=np.array(protein_train)\n",
    "    protein_test=np.array(protein_test)\n",
    "\n",
    "\n",
    "    \n",
    "#For saving and loading the array as an compressed npz file\n",
    "    if '-s' in args and '-r' in args:\n",
    "        print('Do not use save(-s) and read(-r) at the same time')\n",
    "        sys.exit()\n",
    "    elif '-s' in args:\n",
    "        print('-Saving files...')\n",
    "        np.savez_compressed('protein_train', protein_train)\n",
    "        np.savez_compressed('protein_test', protein_test)\n",
    "        np.savez_compressed('protein_target', protein_target)\n",
    "        print('-Files saved as \"protein_train.npz\",\"protein_test.npz\",\"protein_target.npz\"')\n",
    "\n",
    "    elif '-r' in args:\n",
    "        print('-Loading files...')\n",
    "        protein_train = np.load('protein_train.npz')\n",
    "        protein_test = np.load('protein_test.npz')\n",
    "        protein_target = np.load('protein_target.npz')\n",
    "        \n",
    "        for key,array in protein_train.items():\n",
    "            protein_train=protein_train[key]\n",
    "        for key,array in protein_test.items():\n",
    "            protein_test=protein_test[key]\n",
    "        for key,array in protein_target.items():\n",
    "            protein_target=protein_target[key]\n",
    "            \n",
    "            \n",
    "        print('-Files loaded')\n",
    "        \n",
    "    print(len(protein_train))\n",
    "    print(len(protein_target))\n",
    "    print(len(protein_test))\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "    deep_learning(protein_train,protein_test,protein_target)\n",
    "        \n",
    "            \n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
